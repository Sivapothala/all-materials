{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "- Theory\n",
    "    - can be used for classification and regression\n",
    "    - works by recursively partitioning the data into smaller subsets based on the most informative feature.\n",
    "    - Types \n",
    "        - ID3 - can have multiple Splits. \n",
    "        - CART - Only Binary splits\n",
    "    - There is pure and impure split \n",
    "    - pure split has the leaf node.\n",
    "    - to mathematically find the impure split or pur split we use information (Purity)\n",
    "         - Entropy\n",
    "         - Gini Impurity\n",
    "    - what Feature to select for split\n",
    "        - Information Gain\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Entropy | Gini Impurity |\n",
    "| -- | -- |\n",
    "| h(s) = -P+ logP+ - P-logP- | 1 - ∑ (p)^2 i = 1 to n  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Gain (s,f1) = H(s) - ∑|Sv|/|S| * H(Sv)\n",
    "- H(s) - Entropy of the root Node\n",
    "- S(v) - howmany values it got after spliting\n",
    "- s - total values of feature\n",
    "- H(Sv) - Entropy of each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Vs Gini Impurity\n",
    "- whenever dataset is Small => Entropy\n",
    "- Large => Gini Impurity\n",
    "- Most of the time => Gini Impurity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Split for Continuous Values or Numerical Values\n",
    "\n",
    "- Sort the Feature values in Ascending\n",
    "- Create Different Decision trees\n",
    "- Take every value as threshold and calculate the Ouput Values Split Like howmany Yes/No's.\n",
    "- Select Which Split is Good in One Feature From all the Desicion trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Pruning & Pre- Pruning\n",
    "\n",
    "- If you split until the Leaf Nodes for all the features then there is a Problem of Over Fitting.\n",
    "    - Low Bias & High Variance.\n",
    "- Post Pruning\n",
    "    - Build the Entire Tree and then Prun it wrt Depth.\n",
    "    - For Smaller DataSet.\n",
    "- Pre Pruning\n",
    "    - While Consturting only Prun it if not needed using Hyper parameters tuning.\n",
    "    - For Larger Datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor\n",
    "\n",
    "- We Cann't use the Entropy , Gini Impurity and Information Gain\n",
    "- We use Variance Reduction\n",
    "- var red = Var(Root) - ∑Wi*Var(Childs) i 1 to n\n",
    "- Wi = elements it got after spliting / total elements\n",
    "- outpt will be declared by average of the leaf nodes where they belong to"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
